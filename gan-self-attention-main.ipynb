{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1079953,"sourceType":"datasetVersion","datasetId":601280},{"sourceId":165084,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":140456,"modelId":163080}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras import layers, Model\n\n# Step 1: Load the dataset from Kaggle\n!pip install kagglehub\nimport kagglehub\n\n# Download dataset\npath = kagglehub.dataset_download(\"andrewmvd/lung-and-colon-cancer-histopathological-images\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-24T11:59:48.669719Z","iopub.execute_input":"2024-10-24T11:59:48.670920Z","iopub.status.idle":"2024-10-24T12:00:45.238320Z","shell.execute_reply.started":"2024-10-24T11:59:48.670867Z","shell.execute_reply":"2024-10-24T12:00:45.236378Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Requirement already satisfied: kagglehub in /opt/conda/lib/python3.10/site-packages (0.3.1)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from kagglehub) (21.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from kagglehub) (2.32.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from kagglehub) (4.66.4)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->kagglehub) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->kagglehub) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->kagglehub) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->kagglehub) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->kagglehub) (2024.8.30)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport numpy as np\nimport tensorflow as tf\nfrom PIL import Image\nfrom tensorflow.keras.preprocessing.image import img_to_array\nfrom tensorflow.keras.optimizers import Adam\n\n# Paths to dataset directories\nPATH_TO_LUNG_N = '/kaggle/input/lung-and-colon-cancer-histopathological-images/lung_colon_image_set/lung_image_sets/lung_n'\nPATH_TO_LUNG_SCC = '/kaggle/input/lung-and-colon-cancer-histopathological-images/lung_colon_image_set/lung_image_sets/lung_scc'\n\n# Define image size and batch size\nIMG_SIZE = (128, 128)\nBATCH_SIZE = 32\n\n# Constants for loss weights\nLAMBDA_CYCLE = 10.0\nLAMBDA_IDENTITY = 5.0\n\n# Define loss object with from_logits=True\nloss_obj = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n\n# Discriminator loss with updated function\ndef discriminator_loss(real, generated):\n    real_loss = loss_obj(tf.ones_like(real), real)\n    generated_loss = loss_obj(tf.zeros_like(generated), generated)\n    total_disc_loss = real_loss + generated_loss\n    return total_disc_loss * 0.5\n\n# Generator loss function\ndef generator_loss(generated):\n    return loss_obj(tf.ones_like(generated), generated)\n\n# Cycle consistency loss\ndef calc_cycle_loss(real_image, cycled_image):\n    return LAMBDA_CYCLE * tf.reduce_mean(tf.abs(real_image - cycled_image))\n\n# Identity loss function\ndef identity_loss(real_image, same_image):\n    return LAMBDA_IDENTITY * tf.reduce_mean(tf.abs(real_image - same_image))\n\n# Function to load and preprocess images\ndef preprocess_image(img_path):\n    img = Image.open(img_path).convert(\"RGB\")\n    img = img.resize(IMG_SIZE)\n    img = img_to_array(img) / 127.5 - 1\n    return img\n\n# Dataset generator function with added noise in discriminator\ndef load_data(path_n, path_scc, batch_size=BATCH_SIZE):\n    lung_n_images = [os.path.join(path_n, fname) for fname in os.listdir(path_n)]\n    lung_scc_images = [os.path.join(path_scc, fname) for fname in os.listdir(path_scc)]\n\n    def generator():\n        for img_n, img_scc in zip(lung_n_images, lung_scc_images):\n            yield preprocess_image(img_n), preprocess_image(img_scc)\n\n    dataset = tf.data.Dataset.from_generator(generator, output_signature=(\n        tf.TensorSpec(shape=(128, 128, 3), dtype=tf.float32),\n        tf.TensorSpec(shape=(128, 128, 3), dtype=tf.float32),\n    ))\n    return dataset.shuffle(buffer_size=1000).repeat().batch(batch_size)\n\n# Initialize dataset\ndataset = load_data(PATH_TO_LUNG_N, PATH_TO_LUNG_SCC)\nsteps_per_epoch = 500 \n    \n@tf.keras.utils.register_keras_serializable()\nclass SelfAttention(tf.keras.layers.Layer):\n    def __init__(self, channels, trainable=True, **kwargs):\n        super(SelfAttention, self).__init__(trainable=trainable, **kwargs)\n        self.channels = channels\n        self.gamma = tf.Variable(1.0, trainable=trainable, dtype=tf.float32)  # Set gamma to float32\n\n    def build(self, input_shape):\n        self.theta = tf.keras.layers.Conv2D(self.channels // 8, kernel_size=1)\n        self.phi = tf.keras.layers.Conv2D(self.channels // 8, kernel_size=1)\n        self.g = tf.keras.layers.Conv2D(self.channels // 2, kernel_size=1)\n        self.o = tf.keras.layers.Conv2D(self.channels, kernel_size=1)\n        super(SelfAttention, self).build(input_shape)\n\n    def call(self, x):\n        batch_size, height, width, channels = tf.shape(x)[0], tf.shape(x)[1], tf.shape(x)[2], tf.shape(x)[3]\n        \n        theta = tf.reshape(self.theta(x), [batch_size, -1, self.channels // 8])\n        phi = tf.reshape(self.phi(x), [batch_size, -1, self.channels // 8])\n        attention_map = tf.nn.softmax(tf.matmul(theta, phi, transpose_b=True))\n\n        g = tf.reshape(self.g(x), [batch_size, -1, self.channels // 2])\n        attention_out = tf.matmul(attention_map, g)\n        attention_out = tf.reshape(attention_out, [batch_size, height, width, self.channels // 2])\n        attention_out = self.o(attention_out)\n        \n        # Cast attention_out and x to float32 before adding\n        return self.gamma * tf.cast(attention_out, tf.float32) + tf.cast(x, tf.float32)\n\n\n\n# Generator Network with ResNet and Self-Attention blocks\ndef ResNetGenerator(input_shape, filters=64, num_blocks=9):\n    inputs = tf.keras.layers.Input(shape=input_shape)\n\n    x = tf.keras.layers.Conv2D(filters, kernel_size=7, padding='same')(inputs)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.ReLU()(x)\n\n    for _ in range(2):\n        filters *= 2\n        x = tf.keras.layers.Conv2D(filters, kernel_size=3, strides=2, padding='same')(x)\n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.ReLU()(x)\n\n    for _ in range(num_blocks):\n        residual = x\n        x = tf.keras.layers.Conv2D(filters, kernel_size=3, padding='same')(x)\n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.ReLU()(x)\n        x = tf.keras.layers.Conv2D(filters, kernel_size=3, padding='same')(x)\n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.add([x, residual])\n\n        if _ == num_blocks // 2:\n            x = SelfAttention(filters)(x)\n\n    for _ in range(2):\n        filters //= 2\n        x = tf.keras.layers.Conv2DTranspose(filters, kernel_size=3, strides=2, padding='same')(x)\n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.ReLU()(x)\n\n    x = tf.keras.layers.Conv2D(3, kernel_size=7, padding='same', activation='tanh')(x)\n    return tf.keras.Model(inputs, x)\n\n# Discriminator Network with PatchGAN\ndef PatchGANDiscriminator(input_shape, filters=64):\n    inputs = tf.keras.layers.Input(shape=input_shape)\n    x = tf.keras.layers.Conv2D(filters, kernel_size=4, strides=2, padding='same')(inputs)\n    x = tf.keras.layers.LeakyReLU(0.2)(x)\n\n    for i in range(3):\n        filters *= 2\n        stride = 1 if i == 2 else 2\n        x = tf.keras.layers.Conv2D(filters, kernel_size=4, strides=stride, padding='same')(x)\n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.LeakyReLU(0.2)(x)\n\n    x = tf.keras.layers.Conv2D(1, kernel_size=4, padding='same')(x)\n    return tf.keras.Model(inputs, x)\n    \nclass CycleGAN(tf.keras.Model):\n    def __init__(self, input_shape):\n        super(CycleGAN, self).__init__()\n        self.gen_G = ResNetGenerator(input_shape)\n        self.gen_F = ResNetGenerator(input_shape)\n        self.disc_X = PatchGANDiscriminator(input_shape)\n        self.disc_Y = PatchGANDiscriminator(input_shape)\n\n    def compile(self, gen_G_opt, gen_F_opt, disc_X_opt, disc_Y_opt):\n        super(CycleGAN, self).compile()\n        self.gen_G_opt = gen_G_opt\n        self.gen_F_opt = gen_F_opt\n        self.disc_X_opt = disc_X_opt\n        self.disc_Y_opt = disc_Y_opt\n\n    @tf.function\n    def train_step(self, batch_data):\n        real_x, real_y = batch_data\n\n        with tf.GradientTape(persistent=True) as tape:\n            # Generator forward pass\n            fake_y = self.gen_G(real_x, training=True)\n            cycle_x = self.gen_F(fake_y, training=True)\n            fake_x = self.gen_F(real_y, training=True)\n            cycle_y = self.gen_G(fake_x, training=True)\n            \n            same_x = self.gen_F(real_x, training=True)\n            same_y = self.gen_G(real_y, training=True)\n            \n            # Add noise to real and fake images for discriminator inputs\n            noise_factor = 0.05\n            real_x_noisy = real_x + noise_factor * tf.random.normal(shape=tf.shape(real_x))\n            fake_x_noisy = fake_x + noise_factor * tf.random.normal(shape=tf.shape(fake_x))\n            real_y_noisy = real_y + noise_factor * tf.random.normal(shape=tf.shape(real_y))\n            fake_y_noisy = fake_y + noise_factor * tf.random.normal(shape=tf.shape(fake_y))\n\n            # Discriminator forward pass\n            disc_real_x = self.disc_X(real_x_noisy, training=True)\n            disc_fake_x = self.disc_X(fake_x_noisy, training=True)\n            disc_real_y = self.disc_Y(real_y_noisy, training=True)\n            disc_fake_y = self.disc_Y(fake_y_noisy, training=True)\n\n            # Calculate generator losses\n            gen_G_loss = generator_loss(disc_fake_y)\n            gen_F_loss = generator_loss(disc_fake_x)\n            cycle_loss_G = calc_cycle_loss(real_x, cycle_x)\n            cycle_loss_F = calc_cycle_loss(real_y, cycle_y)\n            id_loss_G = identity_loss(real_y, same_y)\n            id_loss_F = identity_loss(real_x, same_x)\n\n            total_gen_G_loss = gen_G_loss + cycle_loss_G + id_loss_G\n            total_gen_F_loss = gen_F_loss + cycle_loss_F + id_loss_F\n\n            # Calculate discriminator losses with updated discriminator_loss\n            disc_X_loss = discriminator_loss(disc_real_x, disc_fake_x)\n            disc_Y_loss = discriminator_loss(disc_real_y, disc_fake_y)\n\n        # Apply gradients to optimizers\n        gradients_G = tape.gradient(total_gen_G_loss, self.gen_G.trainable_variables)\n        gradients_F = tape.gradient(total_gen_F_loss, self.gen_F.trainable_variables)\n        gradients_disc_X = tape.gradient(disc_X_loss, self.disc_X.trainable_variables)\n        gradients_disc_Y = tape.gradient(disc_Y_loss, self.disc_Y.trainable_variables)\n\n        self.gen_G_opt.apply_gradients(zip(gradients_G, self.gen_G.trainable_variables))\n        self.gen_F_opt.apply_gradients(zip(gradients_F, self.gen_F.trainable_variables))\n        self.disc_X_opt.apply_gradients(zip(gradients_disc_X, self.disc_X.trainable_variables))\n        self.disc_Y_opt.apply_gradients(zip(gradients_disc_Y, self.disc_Y.trainable_variables))\n\n        # Return a dictionary of losses to monitor\n        return {\n            \"G_loss\": total_gen_G_loss,\n            \"F_loss\": total_gen_F_loss,\n            \"D_X_loss\": disc_X_loss,\n            \"D_Y_loss\": disc_Y_loss\n        }\n\n\n# Compile and initialize the model\ninput_shape = (128, 128, 3)\ncycle_gan_model = CycleGAN(input_shape)\ncycle_gan_model.compile(\n    gen_G_opt=Adam(learning_rate=0.0001, beta_1=0.5),\n    gen_F_opt=Adam(learning_rate=0.0001, beta_1=0.5),\n    disc_X_opt=Adam(learning_rate=0.0001, beta_1=0.5),\n    disc_Y_opt=Adam(learning_rate=0.0001, beta_1=0.5)\n)\n    \nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\n\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\ndef save_generated_images(epoch, gen_G, gen_F, real_x, real_y, save_dir=\"generated_images\", max_samples=3):\n    fake_y = gen_G(real_x, training=False)\n    fake_x = gen_F(real_y, training=False)\n    cycle_x = gen_F(fake_y, training=False)\n    cycle_y = gen_G(fake_x, training=False)\n\n    # Create directory to save images if it doesn't exist\n    os.makedirs(save_dir, exist_ok=True)\n\n    # Limit the number of samples to display\n    num_samples = min(max_samples, len(real_x))\n\n    # Create a plot with 3 columns: input, generated, and reconstructed\n    fig, axes = plt.subplots(num_samples, 3, figsize=(15, 5 * num_samples))\n\n    for i in range(num_samples):\n        # Convert tensors to NumPy arrays and rescale\n        real_img = (real_x[i].numpy() * 0.5 + 0.5) * 255.0\n        fake_img = (fake_y[i].numpy() * 0.5 + 0.5) * 255.0\n        cycle_img = (cycle_x[i].numpy() * 0.5 + 0.5) * 255.0\n\n        # Convert to uint8 and display in respective columns\n        axes[i, 0].imshow(real_img.astype(np.uint8))\n        axes[i, 0].set_title(\"Input\")\n        axes[i, 1].imshow(fake_img.astype(np.uint8))\n        axes[i, 1].set_title(\"Generated\")\n        axes[i, 2].imshow(cycle_img.astype(np.uint8))\n        axes[i, 2].set_title(\"Reconstructed\")\n\n        for ax in axes[i, :]:\n            ax.axis(\"off\")\n\n    # Save the entire plot as a single image\n    plt.savefig(f\"{save_dir}/epoch_{epoch}_generated_images.png\")\n    plt.close(fig)\n\n\n\n\n# Training loop\nfor epoch in range(100):\n    # Initialize lists to track losses\n    g_losses = []\n    f_losses = []\n    d_x_losses = []\n    d_y_losses = []\n    \n    for i, batch_data in enumerate(dataset.take(steps_per_epoch)):\n        real_x, real_y = batch_data  # Unpack the batch data\n        losses = cycle_gan_model.train_step(batch_data)\n        \n        # Collect losses for each batch\n        g_losses.append(losses['G_loss'])\n        f_losses.append(losses['F_loss'])\n        d_x_losses.append(losses['D_X_loss'])\n        d_y_losses.append(losses['D_Y_loss'])\n        \n\n    # Calculate average losses for the epoch\n    avg_g_loss = np.mean(g_losses)\n    avg_f_loss = np.mean(f_losses)\n    avg_d_x_loss = np.mean(d_x_losses)\n    avg_d_y_loss = np.mean(d_y_losses)\n    \n    # Print average losses for the epoch\n    print(f\"End of Epoch {epoch+1} - Avg G_loss: {avg_g_loss:.4f}, Avg F_loss: {avg_f_loss:.4f}, \"\n          f\"Avg D_X_loss: {avg_d_x_loss:.4f}, Avg D_Y_loss: {avg_d_y_loss:.4f}\")\n    \n    # Save generated images at the end of each epoch\n    save_generated_images(epoch, cycle_gan_model.gen_G, cycle_gan_model.gen_F, real_x, real_y)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T22:45:56.864538Z","iopub.execute_input":"2024-11-12T22:45:56.864898Z","iopub.status.idle":"2024-11-13T08:04:09.788524Z","shell.execute_reply.started":"2024-11-12T22:45:56.864862Z","shell.execute_reply":"2024-11-13T08:04:09.787112Z"}},"outputs":[{"name":"stdout","text":"End of Epoch 1 - Avg G_loss: 6.1132, Avg F_loss: 4.8143, Avg D_X_loss: 0.3940, Avg D_Y_loss: 0.2021\nEnd of Epoch 2 - Avg G_loss: 5.4207, Avg F_loss: 4.3434, Avg D_X_loss: 0.3200, Avg D_Y_loss: 0.1659\nEnd of Epoch 3 - Avg G_loss: 4.7377, Avg F_loss: 4.1709, Avg D_X_loss: 0.3578, Avg D_Y_loss: 0.3082\nEnd of Epoch 4 - Avg G_loss: 3.6742, Avg F_loss: 4.0650, Avg D_X_loss: 0.3787, Avg D_Y_loss: 0.4276\nEnd of Epoch 5 - Avg G_loss: 3.4199, Avg F_loss: 4.1650, Avg D_X_loss: 0.3683, Avg D_Y_loss: 0.4716\nEnd of Epoch 6 - Avg G_loss: 3.0962, Avg F_loss: 3.6546, Avg D_X_loss: 0.4149, Avg D_Y_loss: 0.5237\nEnd of Epoch 7 - Avg G_loss: 3.1455, Avg F_loss: 3.7822, Avg D_X_loss: 0.3879, Avg D_Y_loss: 0.5158\nEnd of Epoch 8 - Avg G_loss: 2.6389, Avg F_loss: 3.4023, Avg D_X_loss: 0.4429, Avg D_Y_loss: 0.6106\nEnd of Epoch 9 - Avg G_loss: 2.4543, Avg F_loss: 3.2223, Avg D_X_loss: 0.4581, Avg D_Y_loss: 0.6323\nEnd of Epoch 10 - Avg G_loss: 2.3894, Avg F_loss: 3.1901, Avg D_X_loss: 0.4561, Avg D_Y_loss: 0.6343\nEnd of Epoch 11 - Avg G_loss: 2.3792, Avg F_loss: 3.0042, Avg D_X_loss: 0.4949, Avg D_Y_loss: 0.6259\nEnd of Epoch 12 - Avg G_loss: 3.1100, Avg F_loss: 3.1873, Avg D_X_loss: 0.4795, Avg D_Y_loss: 0.4850\nEnd of Epoch 13 - Avg G_loss: 2.2910, Avg F_loss: 2.9316, Avg D_X_loss: 0.4867, Avg D_Y_loss: 0.6349\nEnd of Epoch 14 - Avg G_loss: 2.1531, Avg F_loss: 2.8057, Avg D_X_loss: 0.4994, Avg D_Y_loss: 0.6537\nEnd of Epoch 15 - Avg G_loss: 2.1047, Avg F_loss: 2.7514, Avg D_X_loss: 0.4967, Avg D_Y_loss: 0.6535\nEnd of Epoch 16 - Avg G_loss: 2.1555, Avg F_loss: 3.1097, Avg D_X_loss: 0.4437, Avg D_Y_loss: 0.6467\nEnd of Epoch 17 - Avg G_loss: 2.0291, Avg F_loss: 2.5889, Avg D_X_loss: 0.5463, Avg D_Y_loss: 0.6580\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 295\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, batch_data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataset\u001b[38;5;241m.\u001b[39mtake(steps_per_epoch)):\n\u001b[1;32m    294\u001b[0m     real_x, real_y \u001b[38;5;241m=\u001b[39m batch_data  \u001b[38;5;66;03m# Unpack the batch data\u001b[39;00m\n\u001b[0;32m--> 295\u001b[0m     losses \u001b[38;5;241m=\u001b[39m \u001b[43mcycle_gan_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;66;03m# Collect losses for each batch\u001b[39;00m\n\u001b[1;32m    298\u001b[0m     g_losses\u001b[38;5;241m.\u001b[39mappend(losses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mG_loss\u001b[39m\u001b[38;5;124m'\u001b[39m])\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:869\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    866\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    867\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    868\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 869\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    873\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    874\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    875\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1500\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1498\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1500\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1501\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1503\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1504\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1505\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1506\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1508\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1509\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1510\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1514\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1515\u001b[0m   )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"# Directory to save models\nsave_dir = \"saved_models\"\nos.makedirs(save_dir, exist_ok=True)\n\n# Save each model in .keras format\ncycle_gan_model.gen_G.save(os.path.join(save_dir, \"generator_G_02.keras\"))\ncycle_gan_model.gen_F.save(os.path.join(save_dir, \"generator_F_02.keras\"))\ncycle_gan_model.disc_X.save(os.path.join(save_dir, \"discriminator_X_02.keras\"))\ncycle_gan_model.disc_Y.save(os.path.join(save_dir, \"discriminator_Y_02.keras\"))\n\nprint(\"Models saved successfully in .keras format.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T08:04:16.999489Z","iopub.execute_input":"2024-11-13T08:04:16.999925Z","iopub.status.idle":"2024-11-13T08:04:17.720412Z","shell.execute_reply.started":"2024-11-13T08:04:16.999878Z","shell.execute_reply":"2024-11-13T08:04:17.719136Z"}},"outputs":[{"name":"stdout","text":"Models saved successfully in .keras format.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install torch_fidelity","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T09:18:20.527684Z","iopub.execute_input":"2024-11-13T09:18:20.528505Z","iopub.status.idle":"2024-11-13T09:20:50.501189Z","shell.execute_reply.started":"2024-11-13T09:18:20.528467Z","shell.execute_reply":"2024-11-13T09:20:50.500079Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7db1849802e0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/torch-fidelity/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7db18496b190>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/torch-fidelity/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7db18496b550>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/torch-fidelity/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7db184980610>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/torch-fidelity/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7db1849807c0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/torch-fidelity/\u001b[0m\u001b[33m\n\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement torch_fidelity (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for torch_fidelity\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.models import load_model\nfrom skimage.metrics import structural_similarity as ssim\nfrom torch_fidelity import calculate_metrics\nimport torch\n\n# Load trained CycleGAN models\ndef load_cycle_gan_models():\n    gen_G = load_model(\"/kaggle/working/saved_models/generator_G_02.keras\", compile=False)\n    gen_F = load_model(\"/kaggle/working/saved_models/generator_F_02.keras\", compile=False)\n    return gen_G, gen_F\n\n# Convert images from numpy arrays to PyTorch tensors\ndef numpy_to_tensor(images):\n    images = torch.tensor(images).permute(0, 3, 1, 2)  # Change from NHWC to NCHW\n    images = images / 255.0 * 2 - 1  # Normalize to [-1, 1]\n    return images\n\n# Calculate FID score using torch-fidelity\ndef calculate_fid_torch_fidelity(real_images, generated_images):\n    # Convert images to the right format\n    real_images_tensor = numpy_to_tensor(real_images)\n    generated_images_tensor = numpy_to_tensor(generated_images)\n    \n    # Calculate FID using torch-fidelity\n    metrics = calculate_metrics(\n        input1={'torch': real_images_tensor},\n        input2={'torch': generated_images_tensor},\n        fid=True,  # Calculate only FID\n        verbose=False\n    )\n    return metrics['frechet_inception_distance']\n\n# SSIM calculation for 128x128 images or smaller\ndef calculate_ssim(input_images, reconstructed_images, win_size=3):\n    ssim_scores = []\n    for input_img, recon_img in zip(input_images, reconstructed_images):\n        win_size = min(win_size, min(input_img.shape[:2]))  \n        try:\n            score = ssim(input_img, recon_img, multichannel=True, data_range=input_img.max() - input_img.min(), win_size=win_size)\n            ssim_scores.append(score)\n        except ValueError as e:\n            print(f\"Error calculating SSIM for image: {e}\")\n            ssim_scores.append(np.nan)  \n    return np.mean(ssim_scores) * 100 if len(ssim_scores) > 0 else np.nan\n\n# Generate and display images, calculate FID and SSIM scores\ndef generate_and_display_images(gen_G, gen_F, dataset, num_samples=20):\n    input_images, reconstructed_images = [], []\n    fid_scores = []\n\n    for batch in dataset.take(num_samples):\n        real_x, _ = batch\n        fake_y = gen_G(real_x, training=False)\n        cycle_x = gen_F(fake_y, training=False)\n\n        input_images.extend(real_x.numpy())\n        reconstructed_images.extend(cycle_x.numpy())\n\n    input_images = np.array(input_images)\n    reconstructed_images = np.array(reconstructed_images)\n\n    # Display input, generated, and reconstructed images\n    fig, axes = plt.subplots(num_samples, 3, figsize=(15, 5 * num_samples))\n\n    for i in range(num_samples):\n        real_img = (input_images[i] * 0.5 + 0.5) * 255.0\n        cycle_img = (reconstructed_images[i] * 0.5 + 0.5) * 255.0\n\n        axes[i, 0].imshow(real_img.astype(np.uint8))\n        axes[i, 0].set_title(\"Input Image\")\n        axes[i, 2].imshow(cycle_img.astype(np.uint8))\n        axes[i, 2].set_title(\"Reconstructed Image\")\n\n        for ax in axes[i, :]:\n            ax.axis(\"off\")\n\n        ssim_score = calculate_ssim([real_img.astype(np.uint8)], [cycle_img.astype(np.uint8)], win_size=3)\n        fid_score = calculate_fid_torch_fidelity(real_img[None, ...], cycle_img[None, ...])\n        fid_scores.append(fid_score)\n\n        print(f\"Image {i+1}: SSIM = {ssim_score:.4f}, FID = {fid_score:.4f}\")\n\n    plt.tight_layout()\n    plt.show()\n\n    # Display the average FID score for the dataset\n    avg_fid_score = np.mean(fid_scores)\n    print(f\"Average FID for the dataset: {avg_fid_score:.4f}\")\n\n# Load models and initialize dataset\ngen_G, gen_F = load_cycle_gan_models()\ndataset = load_data(PATH_TO_LUNG_N, PATH_TO_LUNG_SCC).take(20)\n\ngenerate_and_display_images(gen_G, gen_F, dataset)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T09:17:53.209506Z","iopub.execute_input":"2024-11-13T09:17:53.210409Z","iopub.status.idle":"2024-11-13T09:17:53.289686Z","shell.execute_reply.started":"2024-11-13T09:17:53.210366Z","shell.execute_reply":"2024-11-13T09:17:53.288498Z"},"_kg_hide-input":true},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[21], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_model\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mskimage\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m structural_similarity \u001b[38;5;28;01mas\u001b[39;00m ssim\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch_fidelity\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m calculate_metrics\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Load trained CycleGAN models\u001b[39;00m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch_fidelity'"],"ename":"ModuleNotFoundError","evalue":"No module named 'torch_fidelity'","output_type":"error"}],"execution_count":21},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}